{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPL71fCHdpkgZjRJ98LTLqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fritzmartin003/RAG-System-Projekt/blob/main/Test_Ichwei%C3%9Fnichtweiter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsv2xqD9MCdx",
        "outputId": "c6d111c8-364d-4a80-a245-96c8ce54ab5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, faiss-cpu, sentence-transformers\n",
            "Successfully installed faiss-cpu-1.10.0 pymupdf-1.25.3 sentence-transformers-3.4.1\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Notwendige Bibliotheken installieren\n",
        "!pip install faiss-cpu transformers sentence-transformers pymupdf numpy scipy\n",
        "!pip uninstall -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import fitz  # PyMuPDF\\n\",\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "sSWSyDmwMPnO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HuggingFaceAPIKey = \"HF2\""
      ],
      "metadata": {
        "id": "KFLbe9FvMnHf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF-Text extrahieren\\n\",\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "      text = \" \"\n",
        "      with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "          text += page.get_text(\"text\") + \" \"\n",
        "        return text\n",
        "\n",
        "pdf_path = \"SakowskiBuch.pdf\"\n",
        "pdf_text = extract_text_from_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "zN48dxaiM3cD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Text in Chunks teilen\\n\",\n",
        "def split_text(text, chunk_size=300, overlap=90):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "      end = start + chunk_size\n",
        "      chunks.append(text[start:end])\n",
        "      start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = split_text(pdf_text)\n",
        "print(f\" PDF in {len(chunks)} Chunks unterteilt!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hzXlnq3NXdt",
        "outputId": "5ee0f37e-07a9-45e2-855e-9c9f7d5c3669"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PDF in 2969 Chunks unterteilt!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import time\n",
        "\n",
        "\n",
        "# Embedding Modell laden\\n\",\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Optimale Batch-Größe finden (Experimentieren!)\\n\",\n",
        "optimal_batch_size = 3000  # Starte mit einem hohen Wert und reduziere, wenn du Speicherprobleme hast\\n\",\n",
        "\n",
        "def create_faiss_index(chunks):\n",
        "      start_time = time.time()  # Zeitmessung\\n\",\n",
        "\n",
        "# Embeddings erstellen (mit optimierter Batch-Verarbeitung und Tensor-Konvertierung)\\n\",\n",
        "      chunk_embeddings_tensor = embedding_model.encode(chunks, batch_size=optimal_batch_size, convert_to_tensor=True)\n",
        "      chunk_embeddings = chunk_embeddings_tensor.cpu().numpy()\n",
        "\n",
        "# Normalisieren der Embeddings (wichtig für die meisten Distanzmetriken)\\n\",\n",
        "      chunk_embeddings = sklearn.preprocessing.normalize(chunk_embeddings)\n",
        "\n",
        "      dimension = chunk_embeddings.shape[1]\n",
        "\n",
        "# FAISS Index erstellen (IndexIVFFlat mit Training)\\n\",\n",
        "      nlist = int(np.sqrt(len(chunk_embeddings)))  # Anzahl der Partitionen (Faustregel)\\n\",\n",
        "      quantizer = faiss.IndexFlatL2(dimension)  # Quantisierer für die Clusterzentren\\n\",\n",
        "      index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n",
        "\n",
        "# Training des Index (NOTWENDIG für IndexIVFFlat!)\\n\",\n",
        "      index.train(chunk_embeddings)\n",
        "\n",
        "# Hinzufügen der Embeddings zum Index (NACH dem Training)\\n\",\n",
        "      index.add(chunk_embeddings)\n",
        "\n",
        "      end_time = time.time()\n",
        "      print(f\"FAISS Vektordatenbank erstellt! (Zeit: {end_time - start_time:.2f} Sekunden)\")\n",
        "      return index\n",
        "\n",
        "# Beispielverwendung (ersetze durch deine tatsächlichen chunks)\\n\",\n",
        "      chunks = [\n",
        "          \"Dies ist ein kurzer Textabschnitt.\",\n",
        "          \"Ein weiterer, etwas längerer Textabschnitt mit mehr Informationen.\",\n",
        "        # ... deine chunks hier ...\\n\",\n",
        "        ],\n",
        "\n",
        "      index = create_faiss_index(chunks)"
      ],
      "metadata": {
        "id": "4NUu40lEOE9g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HuggingFaceAPIKey = \"HF2\"\n",
        "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def get_embedding(text):\n",
        "        return embedding_model.encode(text, convert_to_numpy=True)\n",
        "\n",
        "        chunk_embeddings = np.array(embedding_model.encode(chunks, batch_size=512, convert_to_numpy=True))\n",
        ""
      ],
      "metadata": {
        "id": "nNFaN_gUPfNK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # FAISS Vektordatenbank aufsetzen\\n\",\n",
        "dimension = chunk_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(chunk_embeddings)\n",
        "print(\" FAISS Vektordatenbank erstellt!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gbiWmbs0P1-Z",
        "outputId": "a79ac98d-8f9e-429c-ba4f-9281520260be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'chunk_embeddings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-28660e2b727a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# FAISS Vektordatenbank aufsetzen\\n\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlatL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" FAISS Vektordatenbank erstellt!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'chunk_embeddings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ähnlichkeitssuche in FAISS\\n\",\n",
        "def get_relevant_chunks(query, top_k=3):\n",
        "    query_embedding = get_embedding(query).reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "    return [chunks[i] for i in indices[0]]"
      ],
      "metadata": {
        "id": "Dzs99G7aQTRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import os\n",
        "import torch\n",
        "\n",
        "!huggingface-cli login\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"HF2\"\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Alternativ: \\\"meta-llama/Llama-2-7b-chat-hf\\\"\\n\",\n",
        "\n",
        "# Tokenizer & Modell mit explizitem Token laden\\n\",\n",
        "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
        "\n",
        "\n",
        "def generate_answer(query):\n",
        "    relevant_chunks = get_relevant_chunks(query, top_k=3)\n",
        "    context = \"\\n\".join(relevant_chunks)\n",
        "    prompt = f\"Beantworte die Frage basierend auf diesem Kontext:\\n \\n{context}\\n \\n Frage: {query} \\n Antwort:\"\n",
        "\n",
        "    response = pipe(prompt, max_new_tokens=128, do_sample=True, temperature=0.5)\n",
        "    return response[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "QothueW5Qcc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\\n\",\n",
        "frage = \"Wie hoch ist der gesetzliche Mindestlohn?\"\n",
        "antwort = generate_answer(frage)\n",
        "print(\"Antwort:\", antwort)"
      ],
      "metadata": {
        "id": "1MV0bKozRtRV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}